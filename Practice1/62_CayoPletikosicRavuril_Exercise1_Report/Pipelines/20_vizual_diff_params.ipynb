{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "start_time": "2025-11-10T18:59:49.761081Z",
     "end_time": "2025-11-10T18:59:50.262015Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3m/lmf4h7mj7nz6y587tc98f7sm0000gn/T/ipykernel_53683/1459985524.py:61: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  grid.groupby([\"Dataset\",\"Model\"], group_keys=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Wrote: /Users/vp/PycharmProjects/MachineLearning/Practice1/62_CayoPletikosicRavuril_Exercise1_Report/Pipelines/comparison_outputs/tuned_vs_baseline_comparison.csv\n",
      "✅ Wrote: /Users/vp/PycharmProjects/MachineLearning/Practice1/62_CayoPletikosicRavuril_Exercise1_Report/Pipelines/comparison_outputs/tuned_vs_baseline_summary.txt\n",
      "✅ Wrote: /Users/vp/PycharmProjects/MachineLearning/Practice1/62_CayoPletikosicRavuril_Exercise1_Report/Pipelines/comparison_outputs/tuned_vs_baseline_table.tex\n",
      "\n",
      "--- Quick Summary ---\n",
      "[dataset1 | NeuralNet] Baseline CV-F1=0.7280 → Tuned CV-F1=0.7138 (Δ=-0.0142, -1.95%), Params={\"mlpclassifier__alpha\": 0.001, \"mlpclassifier__early_stopping\": true, \"mlpclassifier__hidden_layer_sizes\": [100, 100], \"mlpclassifier__learning_rate_init\": 0.01, \"mlpclassifier__max_iter\": 300}\n",
      "[dataset1 | RandomForest] Baseline CV-F1=0.8896 → Tuned CV-F1=0.9024 (Δ=+0.0127, +1.43%), Params={\"randomforestclassifier__class_weight\": \"balanced\", \"randomforestclassifier__max_depth\": 20, \"randomforestclassifier__max_features\": \"log2\", \"randomforestclassifier__min_samples_split\": 5, \"randomforestclassifier__n_estimators\": 300}\n",
      "[dataset1 | SVM_rbf] Baseline CV-F1=0.6710 → Tuned CV-F1=0.7483 (Δ=+0.0774, +11.53%), Params={\"svc__C\": 100, \"svc__class_weight\": null, \"svc__gamma\": \"auto\"}\n",
      "[dataset2 | NeuralNet] Baseline CV-F1=0.9999 → Tuned CV-F1=0.9999 (Δ=-0.0000, -0.00%), Params={\"mlpclassifier__alpha\": 0.0001, \"mlpclassifier__early_stopping\": true, \"mlpclassifier__hidden_layer_sizes\": [200], \"mlpclassifier__learning_rate_init\": 0.001, \"mlpclassifier__max_iter\": 300}\n",
      "[dataset2 | RandomForest] Baseline CV-F1=1.0000 → Tuned CV-F1=1.0000 (Δ=+0.0000, +0.00%), Params={\"randomforestclassifier__class_weight\": null, \"randomforestclassifier__max_depth\": null, \"randomforestclassifier__max_features\": null, \"randomforestclassifier__min_samples_split\": 5, \"randomforestclassifier__n_estimators\": 100}\n",
      "[dataset2 | SVM_rbf] Baseline CV-F1=0.9998 → Tuned CV-F1=0.9999 (Δ=+0.0001, +0.01%), Params={\"svc__C\": 100, \"svc__class_weight\": null, \"svc__gamma\": 0.001}\n",
      "[dataset3 | NeuralNet] Baseline CV-F1=1.0000 → Tuned CV-F1=0.9884 (Δ=-0.0116, -1.16%), Params={\"mlpclassifier__alpha\": 0.01, \"mlpclassifier__early_stopping\": true, \"mlpclassifier__hidden_layer_sizes\": [100, 100], \"mlpclassifier__learning_rate_init\": 0.01, \"mlpclassifier__max_iter\": 300}\n",
      "[dataset3 | RandomForest] Baseline CV-F1=0.9931 → Tuned CV-F1=1.0000 (Δ=+0.0069, +0.70%), Params={\"randomforestclassifier__class_weight\": \"balanced\", \"randomforestclassifier__max_depth\": null, \"randomforestclassifier__max_features\": null, \"randomforestclassifier__min_samples_split\": 2, \"randomforestclassifier__n_estimators\": 100}\n",
      "[dataset3 | SVM_rbf] Baseline CV-F1=0.9137 → Tuned CV-F1=0.9885 (Δ=+0.0748, +8.18%), Params={\"svc__C\": 100, \"svc__class_weight\": null, \"svc__gamma\": \"auto\"}\n",
      "[dataset4 | NeuralNet] Baseline CV-F1=0.6762 → Tuned CV-F1=0.6852 (Δ=+0.0091, +1.34%), Params={\"mlpclassifier__alpha\": 0.01, \"mlpclassifier__early_stopping\": true, \"mlpclassifier__hidden_layer_sizes\": [200], \"mlpclassifier__learning_rate_init\": 0.01, \"mlpclassifier__max_iter\": 300}\n",
      "[dataset4 | RandomForest] Baseline CV-F1=0.6804 → Tuned CV-F1=0.7033 (Δ=+0.0229, +3.37%), Params={\"randomforestclassifier__class_weight\": \"balanced\", \"randomforestclassifier__max_depth\": 10, \"randomforestclassifier__max_features\": \"log2\", \"randomforestclassifier__min_samples_split\": 2, \"randomforestclassifier__n_estimators\": 300}\n",
      "[dataset4 | SVM_rbf] Baseline CV-F1=0.6699 → Tuned CV-F1=0.6936 (Δ=+0.0236, +3.53%), Params={\"svc__C\": 1, \"svc__class_weight\": \"balanced\", \"svc__gamma\": \"auto\"}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# =====================================================\n",
    "# Config\n",
    "# =====================================================\n",
    "GRID_RESULTS_CSV = \"all_model_param_results.csv\"  # many rows per (Dataset, Model)\n",
    "BASELINE_SUMMARY_CSV = \"results_summary.csv\"      # one row per (Dataset, Model)\n",
    "PRIMARY_METRIC = \"CV_F1_mean\"                     # selection metric for \"best\"\n",
    "TIEBREAKER_METRIC = \"CV_Accuracy_mean\"            # tiebreaker\n",
    "OUTDIR = Path(\"comparison_outputs\")\n",
    "OUTDIR.mkdir(exist_ok=True)\n",
    "\n",
    "# =====================================================\n",
    "# 1) Load inputs\n",
    "# =====================================================\n",
    "grid = pd.read_csv(GRID_RESULTS_CSV)\n",
    "base = pd.read_csv(BASELINE_SUMMARY_CSV)\n",
    "\n",
    "# Normalize column names (strip spaces)\n",
    "grid.columns = [c.strip() for c in grid.columns]\n",
    "base.columns = [c.strip() for c in base.columns]\n",
    "\n",
    "# Ensure expected columns exist even if missing\n",
    "for col in [\n",
    "    \"Dataset\",\"Model\",\"Params\",\n",
    "    \"CV_Accuracy_mean\",\"CV_Accuracy_std\",\"CV_F1_mean\",\"CV_F1_std\",\n",
    "    \"Fit_time_mean_s\",\"Fit_time_std_s\",\"Score_time_mean_s\",\"Score_time_std_s\",\n",
    "    \"Holdout_Accuracy\",\"Holdout_F1_macro\",\"Holdout_Train_time_s\",\"Holdout_Infer_time_s\",\n",
    "    \"Rank_in_dataset_model\"\n",
    "]:\n",
    "    if col not in grid.columns:\n",
    "        grid[col] = np.nan\n",
    "\n",
    "for col in [\n",
    "    \"Dataset\",\"Model\",\"Accuracy\",\"F1_macro\",\"Train_time_s\",\"Infer_time_s\",\n",
    "    \"CV_Accuracy_mean\",\"CV_Accuracy_std\",\"CV_F1_mean\",\"CV_F1_std\"\n",
    "]:\n",
    "    if col not in base.columns:\n",
    "        base[col] = np.nan\n",
    "\n",
    "# =====================================================\n",
    "# 2) Pick best per (Dataset, Model) from the grid\n",
    "# =====================================================\n",
    "def pick_best(group, primary=PRIMARY_METRIC, tiebreak=TIEBREAKER_METRIC):\n",
    "    g = group.copy()\n",
    "    sort_cols, ascending = [], []\n",
    "    if primary in g.columns:\n",
    "        sort_cols.append(primary); ascending.append(False)   # higher is better\n",
    "    if tiebreak in g.columns:\n",
    "        sort_cols.append(tiebreak); ascending.append(False)\n",
    "    if \"Score_time_mean_s\" in g.columns:\n",
    "        sort_cols.append(\"Score_time_mean_s\"); ascending.append(True)  # prefer faster ties\n",
    "    if not sort_cols:\n",
    "        return g.iloc[[0]]\n",
    "    return g.sort_values(sort_cols, ascending=ascending, na_position=\"last\").iloc[[0]]\n",
    "\n",
    "best = (\n",
    "    grid.groupby([\"Dataset\",\"Model\"], group_keys=False)\n",
    "        .apply(pick_best)\n",
    "        .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# Keep a readable Params column (string)\n",
    "def canonical_params(s):\n",
    "    try:\n",
    "        d = json.loads(s) if isinstance(s, str) else {}\n",
    "        return json.dumps(d, sort_keys=True)\n",
    "    except Exception:\n",
    "        return str(s)\n",
    "\n",
    "best[\"Params_canonical\"] = best[\"Params\"].apply(canonical_params)\n",
    "\n",
    "# Select columns to keep from best\n",
    "best_keep = [\n",
    "    \"Dataset\",\"Model\",\"Params_canonical\",\n",
    "    \"CV_Accuracy_mean\",\"CV_Accuracy_std\",\"CV_F1_mean\",\"CV_F1_std\",\n",
    "    \"Fit_time_mean_s\",\"Fit_time_std_s\",\"Score_time_mean_s\",\"Score_time_std_s\",\n",
    "    \"Holdout_Accuracy\",\"Holdout_F1_macro\",\"Holdout_Train_time_s\",\"Holdout_Infer_time_s\"\n",
    "]\n",
    "best = best[best_keep].rename(columns={\"Params_canonical\": \"Best_Params\"})\n",
    "\n",
    "# =====================================================\n",
    "# 3) Prepare baseline columns (rename for clear suffixes)\n",
    "# =====================================================\n",
    "base_ren = base.rename(columns={\n",
    "    \"CV_Accuracy_mean\":\"Base_CV_Accuracy_mean\",\n",
    "    \"CV_Accuracy_std\":\"Base_CV_Accuracy_std\",\n",
    "    \"CV_F1_mean\":\"Base_CV_F1_mean\",\n",
    "    \"CV_F1_std\":\"Base_CV_F1_std\",\n",
    "    \"Accuracy\":\"Base_Holdout_Accuracy\",\n",
    "    \"F1_macro\":\"Base_Holdout_F1_macro\",\n",
    "    \"Train_time_s\":\"Base_Train_time_s\",\n",
    "    \"Infer_time_s\":\"Base_Infer_time_s\",\n",
    "})\n",
    "\n",
    "base_keep = [\n",
    "    \"Dataset\",\"Model\",\n",
    "    \"Base_CV_Accuracy_mean\",\"Base_CV_Accuracy_std\",\"Base_CV_F1_mean\",\"Base_CV_F1_std\",\n",
    "    \"Base_Holdout_Accuracy\",\"Base_Holdout_F1_macro\",\n",
    "    \"Base_Train_time_s\",\"Base_Infer_time_s\"\n",
    "]\n",
    "base_ren = base_ren[base_keep]\n",
    "\n",
    "# =====================================================\n",
    "# 4) Merge tuned-best vs baseline\n",
    "# =====================================================\n",
    "cmp_df = pd.merge(best, base_ren, on=[\"Dataset\",\"Model\"], how=\"left\")\n",
    "\n",
    "# Compute deltas (tuned minus baseline)\n",
    "cmp_df[\"Delta_CV_F1_mean\"]       = cmp_df[\"CV_F1_mean\"]       - cmp_df[\"Base_CV_F1_mean\"]\n",
    "cmp_df[\"Delta_CV_Accuracy_mean\"] = cmp_df[\"CV_Accuracy_mean\"] - cmp_df[\"Base_CV_Accuracy_mean\"]\n",
    "cmp_df[\"Delta_Holdout_F1_macro\"] = cmp_df[\"Holdout_F1_macro\"] - cmp_df[\"Base_Holdout_F1_macro\"]\n",
    "cmp_df[\"Delta_Holdout_Accuracy\"] = cmp_df[\"Holdout_Accuracy\"] - cmp_df[\"Base_Holdout_Accuracy\"]\n",
    "\n",
    "# Relative (%) improvements (safe divide)\n",
    "def rel_impr(tuned, base):\n",
    "    return np.where(pd.notna(tuned) & pd.notna(base) & (base != 0),\n",
    "                    100.0 * (tuned - base) / base, np.nan)\n",
    "\n",
    "cmp_df[\"Rel_%_CV_F1_mean\"]       = rel_impr(cmp_df[\"CV_F1_mean\"],       cmp_df[\"Base_CV_F1_mean\"])\n",
    "cmp_df[\"Rel_%_CV_Accuracy_mean\"] = rel_impr(cmp_df[\"CV_Accuracy_mean\"], cmp_df[\"Base_CV_Accuracy_mean\"])\n",
    "cmp_df[\"Rel_%_Holdout_F1_macro\"] = rel_impr(cmp_df[\"Holdout_F1_macro\"], cmp_df[\"Base_Holdout_F1_macro\"])\n",
    "cmp_df[\"Rel_%_Holdout_Accuracy\"] = rel_impr(cmp_df[\"Holdout_Accuracy\"], cmp_df[\"Base_Holdout_Accuracy\"])\n",
    "\n",
    "# Flags: did tuning beat baseline?\n",
    "cmp_df[\"Beat_Baseline_on_CV_F1\"]       = cmp_df[\"Delta_CV_F1_mean\"] > 0\n",
    "cmp_df[\"Beat_Baseline_on_CV_Accuracy\"] = cmp_df[\"Delta_CV_Accuracy_mean\"] > 0\n",
    "cmp_df[\"Beat_Baseline_on_Holdout_F1\"]  = cmp_df[\"Delta_Holdout_F1_macro\"] > 0\n",
    "cmp_df[\"Beat_Baseline_on_Holdout_Acc\"] = cmp_df[\"Delta_Holdout_Accuracy\"] > 0\n",
    "\n",
    "# Nice ordering\n",
    "order_cols = [\n",
    "    \"Dataset\",\"Model\",\"Best_Params\",\n",
    "    \"CV_Accuracy_mean\",\"CV_Accuracy_std\",\"CV_F1_mean\",\"CV_F1_std\",\n",
    "    \"Holdout_Accuracy\",\"Holdout_F1_macro\",\n",
    "    \"Base_CV_Accuracy_mean\",\"Base_CV_Accuracy_std\",\"Base_CV_F1_mean\",\"Base_CV_F1_std\",\n",
    "    \"Base_Holdout_Accuracy\",\"Base_Holdout_F1_macro\",\n",
    "    \"Delta_CV_Accuracy_mean\",\"Delta_CV_F1_mean\",\n",
    "    \"Delta_Holdout_Accuracy\",\"Delta_Holdout_F1_macro\",\n",
    "    \"Rel_%_CV_Accuracy_mean\",\"Rel_%_CV_F1_mean\",\n",
    "    \"Rel_%_Holdout_Accuracy\",\"Rel_%_Holdout_F1_macro\",\n",
    "    \"Fit_time_mean_s\",\"Score_time_mean_s\",\"Base_Train_time_s\",\"Base_Infer_time_s\",\n",
    "    \"Beat_Baseline_on_CV_Accuracy\",\"Beat_Baseline_on_CV_F1\",\n",
    "    \"Beat_Baseline_on_Holdout_Acc\",\"Beat_Baseline_on_Holdout_F1\"\n",
    "]\n",
    "cmp_df = cmp_df[order_cols]\n",
    "\n",
    "# Round numeric columns for readability\n",
    "num_cols = cmp_df.select_dtypes(include=[np.number]).columns\n",
    "cmp_df[num_cols] = cmp_df[num_cols].round(4)\n",
    "\n",
    "# Save CSV\n",
    "cmp_csv_path = OUTDIR / \"tuned_vs_baseline_comparison.csv\"\n",
    "cmp_df.to_csv(cmp_csv_path, index=False)\n",
    "print(f\"✅ Wrote: {cmp_csv_path.resolve()}\")\n",
    "\n",
    "# =====================================================\n",
    "# 5) Quick text summary per dataset/model (human-readable)\n",
    "# =====================================================\n",
    "lines = []\n",
    "for (ds, mdl), g in cmp_df.groupby([\"Dataset\",\"Model\"]):\n",
    "    r = g.iloc[0]\n",
    "    lines.append(\n",
    "        f\"[{ds} | {mdl}] \"\n",
    "        f\"Baseline CV-F1={r['Base_CV_F1_mean']:.4f} → Tuned CV-F1={r['CV_F1_mean']:.4f} \"\n",
    "        f\"(Δ={r['Delta_CV_F1_mean']:+.4f}, {r['Rel_%_CV_F1_mean']:+.2f}%), \"\n",
    "        f\"Params={r['Best_Params']}\"\n",
    "    )\n",
    "txt_path = OUTDIR / \"tuned_vs_baseline_summary.txt\"\n",
    "with open(txt_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\\n\".join(lines))\n",
    "print(f\"✅ Wrote: {txt_path.resolve()}\")\n",
    "\n",
    "# =====================================================\n",
    "# 6) (Optional) LaTeX table (short comparison)\n",
    "# =====================================================\n",
    "short_cols = [\n",
    "    \"Dataset\",\"Model\",\n",
    "    \"Base_CV_Accuracy_mean\",\"CV_Accuracy_mean\",\"Delta_CV_Accuracy_mean\",\n",
    "    \"Base_CV_F1_mean\",\"CV_F1_mean\",\"Delta_CV_F1_mean\",\n",
    "    \"Best_Params\"\n",
    "]\n",
    "latex_df = cmp_df[short_cols].copy()\n",
    "# escape underscores in model/dataset names for LaTeX\n",
    "latex_df[\"Dataset\"] = latex_df[\"Dataset\"].astype(str).str.replace(\"_\", \"\\\\_\", regex=False)\n",
    "latex_df[\"Model\"]   = latex_df[\"Model\"].astype(str).str.replace(\"_\", \"\\\\_\", regex=False)\n",
    "\n",
    "latex = latex_df.to_latex(\n",
    "    index=False,\n",
    "    escape=False,\n",
    "    float_format=\"%.4f\",\n",
    "    caption=\"Baseline vs. tuned best per (dataset, model). Deltas are tuned minus baseline.\",\n",
    "    label=\"tab:tuned_vs_baseline\"\n",
    ")\n",
    "\n",
    "with open(OUTDIR / \"tuned_vs_baseline_table.tex\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(latex)\n",
    "\n",
    "print(f\"✅ Wrote: {(OUTDIR / 'tuned_vs_baseline_table.tex').resolve()}\")\n",
    "print(\"\\n--- Quick Summary ---\")\n",
    "print(\"\\n\".join(lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
